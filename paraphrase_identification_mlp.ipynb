{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f64a444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e91a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_with_label.txt',sep='\\t',header=None,quoting=csv.QUOTE_NONE)\n",
    "df_test = pd.read_csv('dev_with_label.txt', sep='\\t',header=None,quoting=csv.QUOTE_NONE)\n",
    "df_test_nolabel = pd.read_csv('test_without_label.txt', sep='\\t',header=None,quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33782f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training datatset shape: (7801, 4)\n",
      "Test datatset shape: (4000, 4)\n",
      "Test no label datatset shape: (4000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training datatset shape: \" + str(df_train.shape))\n",
    "print(\"Test datatset shape: \" + str(df_test.shape))\n",
    "print(\"Test no label datatset shape: \" + str(df_test_nolabel.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07c0254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting column names for test and training set\n",
    "df_train.columns = [\"instance_id\",\"sent1\",\"sent2\",\"gold_score\"]\n",
    "df_test.columns = [\"instance_id\",\"sent1\",\"sent2\",\"gold_score\"]\n",
    "df_test_nolabel.columns = [\"instance_id\",\"sent1\",\"sent2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31099a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance_id    0\n",
      "sent1          0\n",
      "sent2          0\n",
      "gold_score     0\n",
      "dtype: int64\n",
      "instance_id    0\n",
      "sent1          0\n",
      "sent2          0\n",
      "gold_score     0\n",
      "dtype: int64\n",
      "instance_id    0\n",
      "sent1          0\n",
      "sent2          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check for null values and eliminate them\n",
    "print(df_train.isnull().sum())\n",
    "print(df_test.isnull().sum())\n",
    "print(df_test_nolabel.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df4e0f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data target names: [0 1]\n",
      "Test data target names: [0 1]\n"
     ]
    }
   ],
   "source": [
    "#possible target names \n",
    "print(\"Train data target names: {}\".format(df_train[\"gold_score\"].unique()))\n",
    "print(\"Test data target names: {}\".format(df_test[\"gold_score\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89bb5790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance_id    object\n",
      "sent1          object\n",
      "sent2          object\n",
      "gold_score      int64\n",
      "dtype: object\n",
      "instance_id    object\n",
      "sent1          object\n",
      "sent2          object\n",
      "gold_score      int64\n",
      "dtype: object\n",
      "instance_id    object\n",
      "sent1          object\n",
      "sent2          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train.dtypes)\n",
    "print(df_test.dtypes)\n",
    "print(df_test_nolabel.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bfb46842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training dataset\n",
    "df_train['sent1']=df_train['sent1'].astype('string')\n",
    "df_train['sent2']=df_train['sent2'].astype('string')\n",
    "df_train['sent1'] = df_train['sent1'].str.lower()\n",
    "df_train['sent2'] = df_train['sent2'].str.lower()\n",
    "#Test dataset\n",
    "df_test['sent1']=df_test['sent1'].astype('string')\n",
    "df_test['sent2']=df_test['sent2'].astype('string')\n",
    "df_test['sent1'] = df_test['sent1'].str.lower()\n",
    "df_test['sent2'] = df_test['sent2'].str.lower()\n",
    "#Test with no label\n",
    "df_test_nolabel['sent1']=df_test_nolabel['sent1'].astype('string')\n",
    "df_test_nolabel['sent2']=df_test_nolabel['sent2'].astype('string')\n",
    "df_test_nolabel['sent1'] = df_test_nolabel['sent1'].str.lower()\n",
    "df_test_nolabel['sent2'] = df_test_nolabel['sent2'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e47cc1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance_id    object\n",
      "sent1          string\n",
      "sent2          string\n",
      "gold_score      int64\n",
      "dtype: object\n",
      "instance_id    object\n",
      "sent1          string\n",
      "sent2          string\n",
      "gold_score      int64\n",
      "dtype: object\n",
      "instance_id    object\n",
      "sent1          string\n",
      "sent2          string\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train.dtypes)\n",
    "print(df_test.dtypes)\n",
    "print(df_test_nolabel.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9484a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "df_train['sent1'] = df_train['sent1'].str.replace(\",\",\"\")\n",
    "df_train['sent2'] = df_train['sent2'].str.replace(\",\",\"\")\n",
    "#Testing\n",
    "df_test['sent1'] = df_test['sent1'].str.replace(\",\",\"\")\n",
    "df_test['sent2'] = df_test['sent2'].str.replace(\",\",\"\")\n",
    "#Testing with no label\n",
    "df_test_nolabel['sent1'] = df_test_nolabel['sent1'].str.replace(\",\",\"\")\n",
    "df_test_nolabel['sent2'] = df_test_nolabel['sent2'].str.replace(\",\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85559e8b",
   "metadata": {},
   "source": [
    "## Feature1: Word count difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7177f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "df_train['sent1l'] = df_train['sent1'].str.split()\n",
    "df_train['sent2l'] = df_train['sent2'].str.split()\n",
    "#Testing\n",
    "df_test['sent1l'] = df_test['sent1'].str.split()\n",
    "df_test['sent2l'] = df_test['sent2'].str.split()\n",
    "#Testing with no label\n",
    "df_test_nolabel['sent1l'] = df_test_nolabel['sent1'].str.split()\n",
    "df_test_nolabel['sent2l'] = df_test_nolabel['sent2'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a46bd80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "df_train['sent1l']=df_train['sent1l'].apply(lambda x: len(x))\n",
    "df_train['sent2l']=df_train['sent2l'].apply(lambda x: len(x))\n",
    "#Testing\n",
    "df_test['sent1l']=df_test['sent1l'].apply(lambda x: len(x))\n",
    "df_test['sent2l']=df_test['sent2l'].apply(lambda x: len(x))\n",
    "#Testing with no label\n",
    "df_test_nolabel['sent1l']=df_test_nolabel['sent1'].apply(lambda x: len(x))\n",
    "df_test_nolabel['sent2l']=df_test_nolabel['sent2'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "139b5ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.assign(wcd=lambda x: abs((x['sent1l']-x['sent2l'])))\n",
    "df_test = df_test.assign(wcd=lambda x: abs((x['sent1l']-x['sent2l'])))\n",
    "df_test_nolabel = df_test_nolabel.assign(wcd=lambda x: abs((x['sent1l']-x['sent2l'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a7c7df",
   "metadata": {},
   "source": [
    "## Feature 2: Fuzzy ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0b11339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4a0f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['fuzz_ratio'] = df_train.apply(lambda row: fuzz.ratio(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation))), axis = 1)\n",
    "df_test['fuzz_ratio'] = df_test.apply(lambda row: fuzz.ratio(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation))), axis = 1)\n",
    "df_test_nolabel['fuzz_ratio'] = df_test_nolabel.apply(lambda row: fuzz.ratio(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation))), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a93b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['fuzz_token_sort_ratio'] = df_train.apply(lambda row: fuzz.token_sort_ratio(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation))), axis = 1)\n",
    "df_test['fuzz_token_sort_ratio'] = df_test.apply(lambda row: fuzz.token_sort_ratio(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation))), axis = 1)\n",
    "df_test_nolabel['fuzz_token_sort_ratio'] = df_test_nolabel.apply(lambda row: fuzz.token_sort_ratio(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation))), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf9e758",
   "metadata": {},
   "source": [
    "## Feature 3: Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c0c3bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance as lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65b35302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['lev_dist'] = df_train.apply(lambda row: lev(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation))), axis = 1)\n",
    "df_test['lev_dist'] = df_test.apply(lambda row: lev(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation))), axis = 1)\n",
    "df_test_nolabel['lev_dist'] = df_test_nolabel.apply(lambda row: lev(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation))), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43392a0",
   "metadata": {},
   "source": [
    "## Feature 4: Bleu Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cfd83e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a04f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['bleu_score'] = df_train.apply(lambda row: sentence_bleu(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation)),smoothing_function=SmoothingFunction().method4), axis = 1)\n",
    "df_test['bleu_score'] = df_test.apply(lambda row: sentence_bleu(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation)),smoothing_function=SmoothingFunction().method4),axis = 1)\n",
    "df_test_nolabel['bleu_score'] = df_test_nolabel.apply(lambda row: sentence_bleu(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation)),smoothing_function=SmoothingFunction().method4),axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607fc94c",
   "metadata": {},
   "source": [
    "## Feature 5: NIST score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aaf624ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.nist_score import sentence_nist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "785bb9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['nist_score'] = df_train.apply(lambda row: sentence_nist(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation))), axis = 1)\n",
    "df_test['nist_score'] = df_test.apply(lambda row: sentence_nist(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation))),axis = 1)\n",
    "df_test_nolabel['nist_score'] = df_test_nolabel.apply(lambda row: sentence_nist(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation))),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e04488dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance_id               object\n",
      "sent1                     string\n",
      "sent2                     string\n",
      "gold_score                 int64\n",
      "sent1l                     int64\n",
      "sent2l                     int64\n",
      "wcd                        int64\n",
      "fuzz_ratio                 int64\n",
      "fuzz_token_sort_ratio      int64\n",
      "lev_dist                   int64\n",
      "bleu_score               float64\n",
      "nist_score               float64\n",
      "dtype: object\n",
      "instance_id               object\n",
      "sent1                     string\n",
      "sent2                     string\n",
      "gold_score                 int64\n",
      "sent1l                     int64\n",
      "sent2l                     int64\n",
      "wcd                        int64\n",
      "fuzz_ratio                 int64\n",
      "fuzz_token_sort_ratio      int64\n",
      "lev_dist                   int64\n",
      "bleu_score               float64\n",
      "nist_score               float64\n",
      "dtype: object\n",
      "instance_id               object\n",
      "sent1                     string\n",
      "sent2                     string\n",
      "sent1l                     int64\n",
      "sent2l                     int64\n",
      "wcd                        int64\n",
      "fuzz_ratio                 int64\n",
      "fuzz_token_sort_ratio      int64\n",
      "lev_dist                   int64\n",
      "bleu_score               float64\n",
      "nist_score               float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train.dtypes)\n",
    "print(df_test.dtypes)\n",
    "print(df_test_nolabel.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab874570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_val: (7801, 4), test: (4000, 4), test_nolabel: (4000, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train.drop(columns=['instance_id','sent1','sent2','gold_score','sent1l','sent2l']).values\n",
    "X_test = df_test.drop(columns=['instance_id','sent1','sent2','gold_score','sent1l','sent2l']).values\n",
    "y_train = df_train['gold_score'].values\n",
    "y_test =df_test['gold_score'].values\n",
    "X_test_nolabel = df_test_nolabel.drop(columns=['instance_id','sent1','sent2','sent1l','sent2l']).values\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "# X_train_val = X_train_val.reshape(-1,1)\n",
    "# X_test = X_test.reshape(-1,1)\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "X_test = normalizer.fit_transform(X_test)\n",
    "X_test_nolabel = normalizer.fit_transform(X_test_nolabel)\n",
    "print(\"train_val: {}, test: {}, test_nolabel: {}\".format(X_train.shape, X_test.shape, X_test_nolabel.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125e963",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009cbbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Layer Perceptron \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "import numpy as np\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "\n",
    "scaler.fit(X_train)  \n",
    "tr_vect = scaler.transform(X_train)  \n",
    "\n",
    "scaler.fit(dev_vect)\n",
    "dev_vect = scaler.transform(dev_vect)  \n",
    "\n",
    "scaler.fit(test_vect)  \n",
    "test_vect = scaler.transform(test_vect) \n",
    "\n",
    "#GridSearchCV \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#clf = MLPClassifier(activation='logistic', learning_rate='adaptive', random_state=1, max_iter=300).fit(tr_vect, tr_class)\n",
    "#y_pred = clf.predict(dev_vect)\n",
    "\n",
    "#mlp = MLPClassifier(hidden_layer_sizes=(17,), activation='relu', solver='adam', warm_start=True, max_iter=500)\n",
    "parameters = {'solver': ['lbfgs'], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':np.arange(10, 17), 'random_state':[0,1,2,3,4,5,6,7,8,9]}\n",
    "#clf = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
    "#clf.fit(tr_vect, tr_class)\n",
    "\n",
    "#print(mlp.best_params_)\n",
    "\n",
    "clf = MLPClassifier(activation='logistic', alpha = 0.1, hidden_layer_sizes = 13, learning_rate='adaptive', random_state=4, max_iter=100, solver = 'lbfgs').fit(tr_vect, tr_class)\n",
    "predict_train = clf.predict(tr_vect)\n",
    "predict_test = clf.predict(test_vect)\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(tr_class,predict_train))\n",
    "print(classification_report(tr_class,predict_train))\n",
    "\n",
    "# test on dev\n",
    "#print(confusion_matrix(dev_class,predict_test))\n",
    "#print(classification_report(dev_class,predict_test))\n",
    "#print(f1_score(dev_class, predict_test)) \n",
    "\n",
    "prediction = pd.DataFrame()\n",
    "prediction['instance_id'] = test_data['id']\n",
    "prediction['predicted_label'] = predict_test\n",
    "print(prediction)\n",
    "prediction.to_csv('DariiaDragunova_test_result.txt', sep ='\\t', header = None, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "56563d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "272633e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.764, Accuracy: 0.884\n"
     ]
    }
   ],
   "source": [
    "# clfmlp = MLPClassifier(hidden_layer_sizes=(6,5),\n",
    "#                     random_state=5,\n",
    "#                     verbose=False,\n",
    "#                     learning_rate_init=0.01)\n",
    "clfmlp = MLPClassifier(activation='logistic', alpha = 0.1, hidden_layer_sizes = 13, learning_rate='adaptive', random_state=4)\n",
    "\n",
    "clfmlp.fit(X_train, y_train)\n",
    "y_test_pred = clfmlp.predict(X_test)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "print(\"F1 score: {:.3f}, Accuracy: {:.3f}\".format(f1,acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e95c7e",
   "metadata": {},
   "source": [
    "## Prediction for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f512252",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = clfmlp.predict(X_test_nolabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "10e173ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "iterations = df_test_nolabel['instance_id'].to_numpy()\n",
    "# print(dt)\n",
    "print(len(iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5515caae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "# print(y_test_pred)\n",
    "print(len(y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "48921ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test instances for the test result is : 4000\n"
     ]
    }
   ],
   "source": [
    "file = open('AbuHasnatHasib_test_result.txt', 'w') #write to file\n",
    "count = 0\n",
    "for i in range(0,len(iterations)):\n",
    "    file.write(str(iterations[i]) + \"\\t\" + str(y_test_pred[i]) + \"\\n\")\n",
    "    count+=1\n",
    "print(\"Total number of test instances for the test result is : \" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c2469948",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close() #close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74212a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ffd213",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
